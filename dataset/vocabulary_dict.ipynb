{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91387be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shirota/Depth_image_captioning_pub/dataset\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pycocotools.coco import COCO\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3fe29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの保存先\n",
    "fp_train_caption = cwd+'/coco2014/captions_train2014.json'\n",
    "\n",
    "fp_val_caption = cwd+'/coco2014/captions_val2014.json'\n",
    "\n",
    "fp_word_to_id = cwd+'/coco2014/word_to_id.pkl'\n",
    "fp_id_to_word = cwd+'/coco2014/id_to_word.pkl'\n",
    "\n",
    "# キャプションを読み込み\n",
    "train_coco = COCO(fp_train_caption)\n",
    "train_anns_keys = train_coco.anns.keys()\n",
    "\n",
    "val_coco = COCO(fp_val_caption)\n",
    "val_anns_keys = val_coco.anns.keys()\n",
    "\n",
    "\n",
    "# 単語ーID対応表の作成\n",
    "train_coco_token = []\n",
    "for key in train_anns_keys:\n",
    "    caption = train_coco.anns[key]['caption']\n",
    "    tokens = caption.lower().split()\n",
    "    train_coco_token.extend(tokens)#キャプションの文字列が入ったリストを連結していく、[]+[]=[]\n",
    "\n",
    "# validationセットから抽出\n",
    "val_coco_token = []\n",
    "for key in val_anns_keys:\n",
    "    caption = val_coco.anns[key]['caption']\n",
    "    tokens = caption.lower().split()\n",
    "    val_coco_token.extend(tokens)\n",
    "\n",
    "# ピリオド、カンマを削除\n",
    "table = str.maketrans({'.': '',\n",
    "                       ',': ''})\n",
    "for k in range(len(train_coco_token)):\n",
    "    train_coco_token[k] = train_coco_token[k].translate(table)\n",
    "\n",
    "for k in range(len(val_coco_token)):\n",
    "    val_coco_token[k] = val_coco_token[k].translate(table)\n",
    "\n",
    "# 単語ヒストグラムを作成\n",
    "train_coco_token.extend(val_coco_token)#trainとvalidationのトークンを連結\n",
    "token = train_coco_token\n",
    "freq = Counter(token)\n",
    "\n",
    "# 3回以上出現する単語に限定して辞書を作成\n",
    "vocab = [token for token, count in freq.items() if count >= 3]\n",
    "sorted(vocab)\n",
    "\n",
    "# 特殊トークンの追加\n",
    "vocab.append('<start>') # 文章の始まりを表すトークンを追加\n",
    "vocab.append('<end>')  # 文章の終わりを表すトークンを追加\n",
    "vocab.append('<unk>')  # 辞書に無い単語を表すトークンを追加\n",
    "vocab.append('<null>')  # 系列長を揃えるためのトークンを追加\n",
    "\n",
    "# 単語ー単語ID対応表の作成\n",
    "word_to_id = {token: i for i, token in enumerate(vocab)}\n",
    "id_to_word = {i: token for i, token in enumerate(vocab)}\n",
    "\n",
    "# ファイル出力\n",
    "with open(fp_word_to_id, 'wb') as f:\n",
    "    pickle.dump(word_to_id, f)\n",
    "with open(fp_id_to_word, 'wb') as f:\n",
    "    pickle.dump(id_to_word, f)\n",
    "\n",
    "print(f'単語数: {str(len(word_to_id))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testdepth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
